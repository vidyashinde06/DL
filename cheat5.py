# -*- coding: utf-8 -*-
"""cheat5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1shDuAOqHpoo5suLJkmB2zGXv7w3o6ARJ
"""

import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.models import Model

# Load CIFAR-10 dataset and preprocess
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
x_train = tf.image.resize(x_train / 255.0, (64, 64))
x_test =tf.image.resize(x_test / 255.0, (64, 64))

# Load pre-trained VGG16 model, freeze base layers
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))
for layer in base_model.layers:
  layer.trainable = False

base_model.summary()

# Add custom classifier
x = Flatten()(base_model.output)# Flatten the output from the VGG16 base model
x = Dense(64, activation='relu')(x)# Add a fully connected layer with 64 units and ReLU activation
x = Dropout(0.5)(x)# Add a dropout layer to help prevent overfitting, with 50% dropout rate
x = Dense(10, activation='softmax')(x)# Add the output layer with 10 units (one for each CIFAR-10 class) and softmax activation

# Create and compile model
model = Model(inputs=base_model.input, outputs=x)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train and evaluate model
model.fit(x_train, y_train, epochs=1, validation_data=(x_test, y_test), batch_size=64)
test_acc = model.evaluate(x_test, y_test, verbose=0)[1]

# Predict and visualize results
predictions = model.predict(x_test)
plt.imshow(x_test[10])
plt.title(f"Pred: {predictions[10].argmax()}, Actual: {y_test[10][0]}")
plt.show()

print(f'Test accuracy: {test_acc * 100:.2f}%')